{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/aanna7/.conda/envs/mlb2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Loading data...\n",
      "Training Model...\n",
      "Epoch 1: Val Corr = 0.1257, Best = 0.1257\n",
      "Epoch 2: Val Corr = 0.1777, Best = 0.1777\n",
      "Epoch 3: Val Corr = 0.1150, Best = 0.1777\n",
      "Epoch 4: Val Corr = 0.2927, Best = 0.2927\n",
      "Epoch 5: Val Corr = 0.1896, Best = 0.2927\n",
      "Epoch 6: Val Corr = 0.2973, Best = 0.2973\n",
      "Epoch 7: Val Corr = 0.2901, Best = 0.2973\n",
      "Epoch 8: Val Corr = 0.2985, Best = 0.2985\n",
      "Epoch 9: Val Corr = 0.3034, Best = 0.3034\n",
      "Epoch 10: Val Corr = 0.2999, Best = 0.3034\n",
      "Epoch 11: Val Corr = 0.3060, Best = 0.3060\n",
      "Epoch 12: Val Corr = 0.2956, Best = 0.3060\n",
      "Epoch 13: Val Corr = 0.2749, Best = 0.3060\n",
      "Epoch 14: Val Corr = 0.3194, Best = 0.3194\n",
      "Epoch 15: Val Corr = 0.2822, Best = 0.3194\n",
      "Epoch 16: Val Corr = 0.2771, Best = 0.3194\n",
      "Epoch 17: Val Corr = 0.3101, Best = 0.3194\n",
      "Epoch 18: Val Corr = 0.2842, Best = 0.3194\n",
      "Epoch 19: Val Corr = 0.2900, Best = 0.3194\n",
      "Epoch 20: Val Corr = 0.2963, Best = 0.3194\n",
      "Epoch 21: Val Corr = 0.3291, Best = 0.3291\n",
      "Epoch 22: Val Corr = 0.3084, Best = 0.3291\n",
      "Epoch 23: Val Corr = 0.2903, Best = 0.3291\n",
      "Epoch 24: Val Corr = 0.2940, Best = 0.3291\n",
      "Epoch 25: Val Corr = 0.3405, Best = 0.3405\n",
      "Epoch 26: Val Corr = 0.3402, Best = 0.3405\n",
      "Epoch 27: Val Corr = 0.3164, Best = 0.3405\n",
      "Epoch 28: Val Corr = 0.3193, Best = 0.3405\n",
      "Epoch 29: Val Corr = 0.3083, Best = 0.3405\n",
      "Epoch 30: Val Corr = 0.2823, Best = 0.3405\n",
      "Epoch 31: Val Corr = 0.3042, Best = 0.3405\n",
      "Epoch 32: Val Corr = 0.3414, Best = 0.3414\n",
      "Epoch 33: Val Corr = 0.3111, Best = 0.3414\n",
      "Epoch 34: Val Corr = 0.3140, Best = 0.3414\n",
      "Epoch 35: Val Corr = 0.2956, Best = 0.3414\n",
      "Epoch 36: Val Corr = 0.2966, Best = 0.3414\n",
      "Epoch 37: Val Corr = 0.3286, Best = 0.3414\n",
      "Epoch 38: Val Corr = 0.3379, Best = 0.3414\n",
      "Epoch 39: Val Corr = 0.3286, Best = 0.3414\n",
      "Epoch 40: Val Corr = 0.3115, Best = 0.3414\n",
      "Epoch 41: Val Corr = 0.3434, Best = 0.3434\n",
      "Epoch 42: Val Corr = 0.3154, Best = 0.3434\n",
      "Epoch 43: Val Corr = 0.3170, Best = 0.3434\n",
      "Epoch 44: Val Corr = 0.3255, Best = 0.3434\n",
      "Epoch 45: Val Corr = 0.3082, Best = 0.3434\n",
      "Epoch 46: Val Corr = 0.3007, Best = 0.3434\n",
      "Epoch 47: Val Corr = 0.2812, Best = 0.3434\n",
      "Epoch 48: Val Corr = 0.2853, Best = 0.3434\n",
      "Epoch 49: Val Corr = 0.2945, Best = 0.3434\n",
      "Epoch 50: Val Corr = 0.3397, Best = 0.3434\n",
      "Epoch 51: Val Corr = 0.3163, Best = 0.3434\n",
      "Epoch 52: Val Corr = 0.3191, Best = 0.3434\n",
      "Epoch 53: Val Corr = 0.3006, Best = 0.3434\n",
      "Epoch 54: Val Corr = 0.3409, Best = 0.3434\n",
      "Epoch 55: Val Corr = 0.2997, Best = 0.3434\n",
      "Epoch 56: Val Corr = 0.2898, Best = 0.3434\n",
      "Epoch 57: Val Corr = 0.3144, Best = 0.3434\n",
      "Epoch 58: Val Corr = 0.3043, Best = 0.3434\n",
      "Epoch 59: Val Corr = 0.2959, Best = 0.3434\n",
      "Epoch 60: Val Corr = 0.2719, Best = 0.3434\n",
      "Epoch 61: Val Corr = 0.3287, Best = 0.3434\n",
      "Epoch 62: Val Corr = 0.3485, Best = 0.3485\n",
      "Epoch 63: Val Corr = 0.3265, Best = 0.3485\n",
      "Epoch 64: Val Corr = 0.3278, Best = 0.3485\n",
      "Epoch 65: Val Corr = 0.3214, Best = 0.3485\n",
      "Epoch 66: Val Corr = 0.3292, Best = 0.3485\n",
      "Epoch 67: Val Corr = 0.2732, Best = 0.3485\n",
      "Epoch 68: Val Corr = 0.3427, Best = 0.3485\n",
      "Epoch 69: Val Corr = 0.2921, Best = 0.3485\n",
      "Epoch 70: Val Corr = 0.3004, Best = 0.3485\n",
      "Epoch 71: Val Corr = 0.3172, Best = 0.3485\n",
      "Epoch 72: Val Corr = 0.3066, Best = 0.3485\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 1. Correct ESM Implementation for Mutation Scoring\n",
    "class ESMPredictor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_mutation_score(self, sequence_wt, mutation):\n",
    "        wt, pos, mt = mutation[0], int(mutation[1:-1]), mutation[-1]\n",
    "        \n",
    "        # Create masked sequence\n",
    "        masked_seq = sequence_wt[:pos-1] + self.tokenizer.mask_token + sequence_wt[pos:]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(masked_seq, return_tensors=\"pt\").to(device)\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == self.tokenizer.mask_token_id)[1].item()\n",
    "        \n",
    "        # Get logits\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get token IDs\n",
    "        wt_token = self.tokenizer.convert_tokens_to_ids(wt)\n",
    "        mt_token = self.tokenizer.convert_tokens_to_ids(mt)\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        wt_prob = F.softmax(logits[0, mask_token_index], dim=-1)[wt_token].item()\n",
    "        mt_prob = F.softmax(logits[0, mask_token_index], dim=-1)[mt_token].item()\n",
    "        \n",
    "        return mt_prob - wt_prob  # Positive means more likely\n",
    "\n",
    "# 2. Feature Engineering\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, sequence_wt):\n",
    "        self.sequence_wt = sequence_wt\n",
    "        self.esm_predictor = ESMPredictor()\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        self.aa_to_idx = {aa: i for i, aa in enumerate(self.amino_acids)}\n",
    "        \n",
    "    def get_features(self, df):\n",
    "        features = []\n",
    "        for mut in df['mutant']:\n",
    "            wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
    "            \n",
    "            # Positional features\n",
    "            pos_features = [\n",
    "                pos / len(self.sequence_wt),\n",
    "                self.aa_to_idx[wt] / len(self.amino_acids),\n",
    "                self.aa_to_idx[mt] / len(self.amino_acids)\n",
    "            ]\n",
    "            \n",
    "            # ESM score\n",
    "            try:\n",
    "                esm_score = self.esm_predictor.get_mutation_score(self.sequence_wt, mut)\n",
    "            except:\n",
    "                esm_score = 0  # Fallback if scoring fails\n",
    "            \n",
    "            features.append(pos_features + [esm_score])\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "# 3. Model Architecture\n",
    "class FitnessPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)), 0.1)\n",
    "        x = self.dropout(F.leaky_relu(self.bn2(self.fc2(x)), 0.1))\n",
    "        return self.fc3(x).squeeze()\n",
    "\n",
    "# 4. Active Learning Pipeline\n",
    "class ActiveLearningPipeline:\n",
    "    def __init__(self, sequence_wt):\n",
    "        self.sequence_wt = sequence_wt\n",
    "        self.feature_extractor = FeatureExtractor(sequence_wt)\n",
    "        self.queried_mutants = set()\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.train_df = pd.read_csv('train.csv')\n",
    "        self.train_df['sequence'] = self.train_df['mutant'].apply(\n",
    "            lambda x: get_mutated_sequence(x, self.sequence_wt))\n",
    "        \n",
    "        self.test_df = pd.read_csv('test.csv')\n",
    "        self.test_df['sequence'] = self.test_df['mutant'].apply(\n",
    "            lambda x: get_mutated_sequence(x, self.sequence_wt))\n",
    "        \n",
    "        if os.path.exists('queried_mutants.txt'):\n",
    "            with open('queried_mutants.txt', 'r') as f:\n",
    "                self.queried_mutants.update(line.strip() for line in f)\n",
    "    \n",
    "    def train_model(self):\n",
    "        X = self.feature_extractor.get_features(self.train_df)\n",
    "        y = self.train_df['DMS_score'].values\n",
    "        \n",
    "        # Train/validation split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=SEED)\n",
    "        \n",
    "        # Feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = FitnessPredictor(X_train.shape[1]).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        criterion = nn.HuberLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        best_corr = -1\n",
    "        for epoch in range(100):\n",
    "            model.train()\n",
    "            for i in range(0, len(X_train), 32):\n",
    "                batch_X = torch.FloatTensor(X_train[i:i+32]).to(device)\n",
    "                batch_y = torch.FloatTensor(y_train[i:i+32]).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(torch.FloatTensor(X_val).to(device)).cpu().numpy()\n",
    "                corr = spearmanr(y_val, val_preds)[0]\n",
    "                \n",
    "                if corr > best_corr:\n",
    "                    best_corr = corr\n",
    "                    torch.save(model.state_dict(), 'best_model_b.pth')\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Val Corr = {corr:.4f}, Best = {best_corr:.4f}\")\n",
    "        \n",
    "        model.load_state_dict(torch.load('best_model_b.pth'))\n",
    "        return model\n",
    "    \n",
    "    def select_queries(self, model, n_queries=100):\n",
    "        X_test = self.scaler.transform(\n",
    "            self.feature_extractor.get_features(self.test_df))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_preds = model(torch.FloatTensor(X_test).to(device)).cpu().numpy()\n",
    "        \n",
    "        candidates = []\n",
    "        for i, (mut, pred) in enumerate(zip(self.test_df['mutant'], test_preds)):\n",
    "            if mut not in self.queried_mutants:\n",
    "                esm_score = X_test[i, -1]  # ESM score is the last feature\n",
    "                combined_score = 0.7 * pred + 0.3 * esm_score\n",
    "                candidates.append((mut, combined_score))\n",
    "        \n",
    "        # Select top candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected = [mut for mut, score in candidates[:n_queries]]\n",
    "        \n",
    "        # Update queried mutants\n",
    "        self.queried_mutants.update(selected)\n",
    "        with open('query.txt', 'w') as f:\n",
    "            for mut in selected:\n",
    "                f.write(f\"{mut}\\n\")\n",
    "        with open('queried_mutants.txt', 'w') as f:\n",
    "            for mut in self.queried_mutants:\n",
    "                f.write(f\"{mut}\\n\")\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def run_cycle(self):\n",
    "        print('Loading data...')\n",
    "        self.load_data()\n",
    "        print('Training Model...')\n",
    "        model = self.train_model()\n",
    "        print('Selecting Queries...')\n",
    "        queries = self.select_queries(model)\n",
    "        \n",
    "        # Save predictions\n",
    "        X_test = self.scaler.transform(\n",
    "            self.feature_extractor.get_features(self.test_df))\n",
    "        with torch.no_grad():\n",
    "            test_preds = model(torch.FloatTensor(X_test).to(device)).cpu().numpy()\n",
    "        \n",
    "        # pd.DataFrame({\n",
    "        #     'mutant': self.test_df['mutant'],\n",
    "        #     'DMS_score_predicted': test_preds\n",
    "        # }).to_csv('predictions.csv', index=False)\n",
    "\n",
    "# Helper function\n",
    "def get_mutated_sequence(mut, sequence_wt):\n",
    "    wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
    "    return sequence_wt[:pos-1] + mt + sequence_wt[pos:]\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    with open('sequence.fasta', 'r') as f:\n",
    "        sequence_wt = f.readlines()[1].strip()\n",
    "    \n",
    "    pipeline = ActiveLearningPipeline(sequence_wt)\n",
    "    pipeline.run_cycle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
